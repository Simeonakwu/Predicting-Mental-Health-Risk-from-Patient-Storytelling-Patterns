{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Depression Detection with BERT: Complete Tutorial\n",
    "\n",
    "This notebook demonstrates the complete workflow for depression detection from DAIC-WOZ interview transcripts using BERT-based modeling.\n",
    "\n",
    "## Overview\n",
    "1. Data Loading and Preprocessing\n",
    "2. Model Training\n",
    "3. Evaluation\n",
    "4. SHAP Explainability\n",
    "5. Fairness Auditing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from data.data_processor import DAICWOZDataProcessor, load_sample_data\n",
    "from models.bert_model import DepressionDetectionModel\n",
    "from explainability.shap_explainer import ModelExplainer\n",
    "from fairness.fairness_auditor import FairnessAuditor, create_synthetic_sensitive_attributes\n",
    "from utils.evaluation import ModelEvaluator\n",
    "from utils.visualization import plot_label_distribution\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample data\n",
    "print(\"Loading sample data...\")\n",
    "transcripts, labels = load_sample_data()\n",
    "\n",
    "print(f\"Loaded {len(transcripts)} transcripts\")\n",
    "print(f\"Labels distribution: {np.bincount(labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create structured dataset\n",
    "processor = DAICWOZDataProcessor(data_dir=\"../data\")\n",
    "df = processor.create_dataset(transcripts, labels)\n",
    "\n",
    "# Display first few rows\n",
    "print(\"Dataset preview:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize label distribution\n",
    "plot_label_distribution(df['label'].values, title=\"Depression Label Distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "train_df, val_df, test_df = processor.split_dataset(df, train_size=0.6, val_size=0.2, test_size=0.2)\n",
    "\n",
    "print(f\"Train set: {len(train_df)} samples\")\n",
    "print(f\"Validation set: {len(val_df)} samples\")\n",
    "print(f\"Test set: {len(test_df)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "print(\"Initializing BERT model...\")\n",
    "model = DepressionDetectionModel(\n",
    "    model_name='bert-base-uncased',\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "print(f\"Model device: {model.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data loaders\n",
    "train_loader = model.prepare_data(\n",
    "    texts=train_df['cleaned_transcript'].tolist(),\n",
    "    labels=train_df['label'].tolist(),\n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "val_loader = model.prepare_data(\n",
    "    texts=val_df['cleaned_transcript'].tolist(),\n",
    "    labels=val_df['label'].tolist(),\n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "print(\"Data loaders prepared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.model.parameters(), lr=2e-5)\n",
    "\n",
    "# Training loop (simplified for demonstration)\n",
    "num_epochs = 3\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    train_loss = model.train_step(train_loader, optimizer, criterion)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validation\n",
    "    val_loss, val_acc = model.evaluate(val_loader, criterion)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set\n",
    "test_texts = test_df['cleaned_transcript'].tolist()\n",
    "test_labels = test_df['label'].values\n",
    "\n",
    "predictions, probabilities = model.predict(test_texts, batch_size=8)\n",
    "\n",
    "print(f\"Predictions shape: {predictions.shape}\")\n",
    "print(f\"Probabilities shape: {probabilities.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics\n",
    "evaluator = ModelEvaluator()\n",
    "metrics = evaluator.calculate_metrics(test_labels, predictions, probabilities)\n",
    "\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric:15s}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "evaluator.plot_confusion_matrix(test_labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve\n",
    "evaluator.plot_roc_curve(test_labels, probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. SHAP Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize explainer\n",
    "explainer = ModelExplainer(\n",
    "    model=model.model,\n",
    "    tokenizer=model.tokenizer,\n",
    "    device=model.device\n",
    ")\n",
    "\n",
    "# Initialize with background samples\n",
    "background_texts = train_df['cleaned_transcript'].sample(min(10, len(train_df))).tolist()\n",
    "explainer.initialize_explainer(background_texts)\n",
    "\n",
    "print(\"Explainer initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain a sample prediction\n",
    "sample_text = test_texts[0]\n",
    "print(f\"Sample text: {sample_text[:200]}...\")\n",
    "\n",
    "shap_values = explainer.explain_prediction(sample_text, show_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "batch_shap = explainer.explain_batch(test_texts[:5])\n",
    "importance_df = explainer.get_feature_importance(batch_shap, top_k=15)\n",
    "\n",
    "print(\"\\nTop Important Features:\")\n",
    "importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fairness Auditing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic sensitive attributes for demonstration\n",
    "sensitive_attrs = create_synthetic_sensitive_attributes(len(test_labels))\n",
    "\n",
    "print(\"Sensitive attributes created:\")\n",
    "for attr_name, values in sensitive_attrs.items():\n",
    "    print(f\"  {attr_name}: {np.unique(values, return_counts=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform fairness audit\n",
    "auditor = FairnessAuditor()\n",
    "audit_results = auditor.audit_model(\n",
    "    y_true=test_labels,\n",
    "    y_pred=predictions,\n",
    "    sensitive_attributes={'gender': sensitive_attrs['gender']},\n",
    "    threshold=0.1\n",
    ")\n",
    "\n",
    "print(\"\\nFairness Audit Results:\")\n",
    "print(\"Overall Metrics:\", audit_results['overall_metrics'])\n",
    "print(\"\\nFairness Metrics:\", audit_results['fairness_metrics'])\n",
    "print(\"\\nFairness Assessment:\", audit_results['fairness_assessment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot group comparison\n",
    "auditor.plot_group_comparison('gender')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Model and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model.save_model('../models/trained_model')\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation results\n",
    "results_df = pd.DataFrame({\n",
    "    'true_label': test_labels,\n",
    "    'predicted_label': predictions,\n",
    "    'probability_depression': probabilities[:, 1]\n",
    "})\n",
    "\n",
    "results_df.to_csv('../data/processed/test_results.csv', index=False)\n",
    "print(\"Results saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated:\n",
    "- Loading and preprocessing DAIC-WOZ transcripts\n",
    "- Training a BERT-based depression detection model\n",
    "- Evaluating model performance\n",
    "- Explaining predictions using SHAP\n",
    "- Auditing model fairness across demographic groups\n",
    "\n",
    "For more details, refer to the project documentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
